{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, Softmax, Reshape, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from ei_tensorflow.constrained_object_detection import models, dataset, metrics, util\n",
    "from ei_tensorflow.velo import train_keras_model_with_velo\n",
    "from ei_shared.pretrained_weights import get_or_download_pretrained_weights\n",
    "import ei_tensorflow.training\n",
    "\n",
    "WEIGHTS_PREFIX = os.environ.get('WEIGHTS_PREFIX', os.getcwd())\n",
    "\n",
    "def build_model(input_shape: tuple, weights: str, alpha: float,\n",
    "                num_classes: int) -> tf.keras.Model:\n",
    "    \"\"\" Construct a constrained object detection model.\n",
    "\n",
    "    Args:\n",
    "        input_shape: Passed to MobileNet construction.\n",
    "        weights: Weights for initialization of MobileNet where None implies\n",
    "            random initialization.\n",
    "        alpha: MobileNet alpha value.\n",
    "        num_classes: Number of classes, i.e. final dimension size, in output.\n",
    "\n",
    "    Returns:\n",
    "        Uncompiled keras model.\n",
    "\n",
    "    Model takes (B, H, W, C) input and\n",
    "    returns (B, H//8, W//8, num_classes) logits.\n",
    "    \"\"\"\n",
    "\n",
    "    # First create full mobile_net_V2 from (HW, HW, C) input\n",
    "    # to (HW/8, HW/8, C) output\n",
    "    mobile_net_v2 = MobileNetV2(input_shape=input_shape,\n",
    "                                weights=weights,\n",
    "                                alpha=alpha,\n",
    "                                include_top=True)\n",
    "    # Default batch norm is configured for huge networks, let's speed it up\n",
    "    for layer in mobile_net_v2.layers:\n",
    "        if isinstance(layer, BatchNormalization):\n",
    "            layer.momentum = 0.9\n",
    "    # Cut MobileNet where it hits 1/8th input resolution; i.e. (HW/8, HW/8, C)\n",
    "    cut_point = mobile_net_v2.get_layer('block_6_expand_relu')\n",
    "    # Now attach a small additional head on the MobileNet\n",
    "    model = Conv2D(filters=32, kernel_size=1, strides=1,\n",
    "                   activation='relu', name='head')(cut_point.output)\n",
    "    # Adding Dropout layer to avoid overfitting\n",
    "    model = Dropout(0.5)(model)\n",
    "    logits = Conv2D(filters=num_classes, kernel_size=1, strides=1,\n",
    "                    activation=None, name='logits')(model)\n",
    "    return Model(inputs=mobile_net_v2.input, outputs=logits)\n",
    "\n",
    "def train(num_classes: int, learning_rate: float, num_epochs: int,\n",
    "          alpha: float, object_weight: float,\n",
    "          train_dataset: tf.data.Dataset,\n",
    "          validation_dataset: tf.data.Dataset,\n",
    "          best_model_path: str,\n",
    "          input_shape: tuple,\n",
    "          batch_size: int,\n",
    "          use_velo: bool = False,\n",
    "          ensure_determinism: bool = False) -> tf.keras.Model:\n",
    "    \"\"\" Construct and train a constrained object detection model.\n",
    "\n",
    "    Args:\n",
    "        num_classes: Number of classes in datasets. This does not include\n",
    "            implied background class introduced by segmentation map dataset\n",
    "            conversion.\n",
    "        learning_rate: Learning rate for Adam.\n",
    "        num_epochs: Number of epochs passed to model.fit\n",
    "        alpha: Alpha used to construct MobileNet. Pretrained weights will be\n",
    "            used if there is a matching set.\n",
    "        object_weight: The weighting to give the object in the loss function\n",
    "            where background has an implied weight of 1.0.\n",
    "        train_dataset: Training dataset of (x, (bbox, one_hot_y))\n",
    "        validation_dataset: Validation dataset of (x, (bbox, one_hot_y))\n",
    "        best_model_path: location to save best model path. note: weights\n",
    "            will be restored from this path based on best val_f1 score.\n",
    "        input_shape: The shape of the model's input\n",
    "        batch_size: Training batch size\n",
    "        ensure_determinism: If true, functions that may be non-\n",
    "            deterministic are disabled (e.g. autotuning prefetch). This\n",
    "            should be true in test environments.\n",
    "    Returns:\n",
    "        Trained keras model.\n",
    "\n",
    "    Constructs a new constrained object detection model with num_classes+1\n",
    "    outputs (denoting the classes with an implied background class of 0).\n",
    "    Both training and validation datasets are adapted from\n",
    "    (x, (bbox, one_hot_y)) to (x, segmentation_map). Model is trained with a\n",
    "    custom weighted cross entropy function.\n",
    "    \"\"\"\n",
    "\n",
    "    num_classes_with_background = num_classes + 1\n",
    "\n",
    "    width, height, input_num_channels = input_shape\n",
    "    if width != height:\n",
    "        raise Exception(f\"Only square inputs are supported; not {input_shape}\")\n",
    "\n",
    "    # Use pretrained weights, if we have them for configured\n",
    "    allowed_combinations = [{'num_channels': 1, 'alpha': 0.1},\n",
    "                            {'num_channels': 1, 'alpha': 0.35},\n",
    "                            {'num_channels': 3, 'alpha': 0.1},\n",
    "                            {'num_channels': 3, 'alpha': 0.35}]\n",
    "    weights = get_or_download_pretrained_weights(WEIGHTS_PREFIX, input_num_channels, alpha, allowed_combinations)\n",
    "\n",
    "    model = build_model(\n",
    "        input_shape=input_shape,\n",
    "        weights=weights,\n",
    "        alpha=alpha,\n",
    "        num_classes=num_classes_with_background\n",
    "    )\n",
    "\n",
    "    # Derive output size from model\n",
    "    model_output_shape = model.layers[-1].output.shape\n",
    "    _batch, width, height, num_classes = model_output_shape\n",
    "    if width != height:\n",
    "        raise Exception(f\"Only square outputs are supported; not {model_output_shape}\")\n",
    "    output_width_height = width\n",
    "\n",
    "    # Build weighted cross entropy loss specific to this model size\n",
    "    weighted_xent = models.construct_weighted_xent_fn(model.output.shape, object_weight)\n",
    "\n",
    "    prefetch_policy = 1 if ensure_determinism else tf.data.experimental.AUTOTUNE\n",
    "\n",
    "    # Transform bounding box labels into segmentation maps\n",
    "    def as_segmentation(ds, shuffle):\n",
    "        ds = ds.map(dataset.bbox_to_segmentation(output_width_height, num_classes_with_background))\n",
    "        if not ensure_determinism and shuffle:\n",
    "            ds = ds.shuffle(buffer_size=batch_size*4)\n",
    "        ds = ds.batch(batch_size, drop_remainder=False).prefetch(prefetch_policy)\n",
    "        return ds\n",
    "\n",
    "    train_segmentation_dataset = as_segmentation(train_dataset, True)\n",
    "    validation_segmentation_dataset = as_segmentation(validation_dataset, False)\n",
    "\n",
    "    validation_dataset_for_callback = (validation_dataset\n",
    "        .batch(batch_size, drop_remainder=False)\n",
    "        .prefetch(prefetch_policy))\n",
    "\n",
    "    # Initialise bias of final classifier based on training data prior.\n",
    "    util.set_classifier_biases_from_dataset(\n",
    "        model, train_segmentation_dataset)\n",
    "\n",
    "    if not use_velo:\n",
    "        model.compile(loss=weighted_xent,\n",
    "                      optimizer=Adam(learning_rate=learning_rate))\n",
    "\n",
    "    # Create callback that will do centroid scoring on end of epoch against\n",
    "    # validation data. Include a callback to show % progress in slow cases.\n",
    "    callbacks = [\n",
    "        metrics.CentroidScoring(validation_dataset_for_callback,\n",
    "                                output_width_height, num_classes_with_background),\n",
    "        metrics.PrintPercentageTrained(num_epochs),\n",
    "        tf.keras.callbacks.ModelCheckpoint(best_model_path,\n",
    "                                           monitor='val_f1', save_best_only=True, mode='max',\n",
    "                                           save_weights_only=True, verbose=0)\n",
    "    ]\n",
    "\n",
    "    if use_velo:\n",
    "        from tensorflow.python.framework.errors_impl import ResourceExhaustedError\n",
    "        try:\n",
    "            train_keras_model_with_velo(\n",
    "                model,\n",
    "                train_segmentation_dataset,\n",
    "                validation_segmentation_dataset,\n",
    "                loss_fn=weighted_xent,\n",
    "                num_epochs=num_epochs,\n",
    "                callbacks=callbacks\n",
    "            )\n",
    "        except ResourceExhaustedError as e:\n",
    "            print(str(e))\n",
    "            raise Exception(\n",
    "                \"ResourceExhaustedError caught during train_keras_model_with_velo.\"\n",
    "                \" Though VeLO encourages a large batch size, the current\"\n",
    "                f\" size of {batch_size} may be too large. Please try a lower\"\n",
    "                \" value. For further assistance please contact support\"\n",
    "                \" at https://forum.edgeimpulse.com/\")\n",
    "    else:\n",
    "        model.fit(train_segmentation_dataset,\n",
    "                  validation_data=validation_segmentation_dataset,\n",
    "                  epochs=num_epochs, callbacks=callbacks, verbose=0)\n",
    "\n",
    "    # Restore best weights.\n",
    "    model.load_weights(best_model_path)\n",
    "\n",
    "    # Add explicit softmax layer before export.\n",
    "    softmax_layer = Softmax()(model.layers[-1].output)\n",
    "    model = Model(model.input, softmax_layer)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "EPOCHS = args.epochs or 80\n",
    "LEARNING_RATE = args.learning_rate or 0.01\n",
    "BATCH_SIZE = args.batch_size or 32\n",
    "\n",
    "model = train(num_classes=classes,\n",
    "              learning_rate=LEARNING_RATE,\n",
    "              num_epochs=EPOCHS,\n",
    "              alpha=0.35,\n",
    "              object_weight=100,\n",
    "              train_dataset=train_dataset,\n",
    "              validation_dataset=validation_dataset,\n",
    "              best_model_path=BEST_MODEL_PATH,\n",
    "              input_shape=MODEL_INPUT_SHAPE,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              use_velo=False,\n",
    "              ensure_determinism=ensure_determinism)\n",
    "\n",
    "disable_per_channel_quantization = False\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
